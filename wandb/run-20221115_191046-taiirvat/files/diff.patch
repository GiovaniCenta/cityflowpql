diff --git a/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc b/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc
index 904751d..0040281 100644
Binary files a/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc and b/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc differ
diff --git a/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc b/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc
index 7521823..9514c0a 100644
Binary files a/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc and b/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc differ
diff --git a/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc b/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc
index cd5983c..80e70ff 100644
Binary files a/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc and b/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc differ
diff --git a/deepseatreasure-pareto/agent.py b/deepseatreasure-pareto/agent.py
index c689b43..8432966 100644
--- a/deepseatreasure-pareto/agent.py
+++ b/deepseatreasure-pareto/agent.py
@@ -39,20 +39,23 @@ class Pareto():
         
 
     def initializeState(self):
-        state = self.env.reset()
-        s = ''.join(str(state))
-        if s not in self.stateList:
-            self.stateList.append(s)
-        s = self.stateList.index(s)
-        
-        
-        
+        return self.flatten_observation(self.env.reset())
 
+    def flatten_observation(self, obs):
+        #print(obs[1])
         
-        return {'observation':s,'terminal':False}
+        if type(obs[1]) is dict:
+        	return int(np.ravel_multi_index((0,0), (11, 11)))
+        #print(type(obs[1]))
+           
+        else:
+            return int(np.ravel_multi_index(obs, (11, 11)))
 
 
     def train(self,max_episodes,max_steps):
+        
+        metrics.setup_wandb("pql", "pql")
+
         numberOfEpisodes = 0
         episodeSteps = 0
 
@@ -63,20 +66,20 @@ class Pareto():
 
             acumulatedRewards = [0,0]
             episodeSteps = 0
-
+            terminal = False
             #line 3 -> initialize state s
             s = self.initializeState()
             #print(s)
             
             
             #line 4 and 11 -> repeat until s is terminal:
-            while s['terminal'] is not True and episodeSteps < max_steps:
+            while terminal is not True and episodeSteps < max_steps:
                 #env.render()
-                s = self.step(s)
+                s,terminal,reward = self.step(s)
                 #print(s, episodeSteps)
                 episodeSteps += 1
-                acumulatedRewards[0] += s['reward'][0]
-                acumulatedRewards[1] += s['reward'][1]
+                acumulatedRewards[0] += reward[0]
+                acumulatedRewards[1] += reward[1]
 
             metrics.rewards1.append(acumulatedRewards[0])
             metrics.rewards2.append(acumulatedRewards[1])
@@ -87,8 +90,8 @@ class Pareto():
         metrics.pdict = self.polDict
 
 
-    def step(self,state):
-        s = state['observation']
+    def step(self,s):
+        
         
         
 
@@ -105,12 +108,11 @@ class Pareto():
 
         #line 6 ->Take action a and observe state s0 ∈ S and reward vector r ∈ R
         next_state, reward, terminal, _ = self.env.step(action)
-        next_s = ''.join(str(next_state))
+        next_s = self.flatten_observation(next_state)
+        
+        
         
-        if next_s not in self.stateList:
-            self.stateList.append(next_s)
         
-        next_s = self.stateList.index(next_s)
         nd = self.update_non_dominated(s, action, next_s)
         
         
@@ -120,10 +122,9 @@ class Pareto():
         self.avg_r[s, action] += (reward - self.avg_r[s, action]) / self.n_visits[s, action]
 
         self.actionsMethods.epsilon *= self.actionsMethods.epsilonDecrease
+        print(self.actionsMethods.epsilon)
 
-        return {'observation': next_s,
-                'terminal': terminal,
-                'reward': reward}
+        return next_s,terminal,reward
 
     
     def compute_q_set(self, s):
@@ -131,12 +132,14 @@ class Pareto():
         for a in range(self.env.nA):
             nd_sa = self.non_dominated[s][a]
             rew = self.avg_r[s, a]
+            
+          
             q_set.append([rew + self.gamma*nd for nd in nd_sa])
         return np.array(q_set)
 
 
-    def update_non_dominated(self, s, a, s_n):
-        q_set_n = self.compute_q_set(s_n)
+    def update_non_dominated(self, s, a, next_state):
+        q_set_n = self.compute_q_set(next_state)
         # update for all actions, flatten
         solutions = np.concatenate(q_set_n, axis=0)
 
@@ -172,8 +175,10 @@ class actionMethods():
             q_values[i] = hv.compute(ref*-1)
         return q_values
 
+
     def get_non_dominated(self,solutions):
         is_efficient = np.ones(solutions.shape[0], dtype=bool)
+        
         for i, c in enumerate(solutions):
             if is_efficient[i]:
                 # Remove dominated points, will also remove itself
diff --git a/deepseatreasure-pareto/metrics.py b/deepseatreasure-pareto/metrics.py
index 8ca2346..86fc63e 100644
--- a/deepseatreasure-pareto/metrics.py
+++ b/deepseatreasure-pareto/metrics.py
@@ -4,6 +4,8 @@ from re import X
 import matplotlib.pyplot as plt
 import datetime
 import os
+import wandb
+from torch.utils.tensorboard import SummaryWriter
 
 class metrics():
     def __init__(self, episodes, rewards1, rewards2):
@@ -144,4 +146,32 @@ class metrics():
         self.plot_p_front(self.xA3,self.yA3,3)
         
         
-    
\ No newline at end of file
+    def setup_wandb(self, project_name: str, experiment_name: str):
+        self.experiment_name = experiment_name
+        import wandb
+
+        wandb.init(
+            project=project_name,
+            sync_tensorboard=True,
+            config=self.get_config(),
+            name=self.experiment_name,
+            monitor_gym=True,
+            save_code=True,
+
+        )
+        self.writer = SummaryWriter(f"{self.experiment_name}")
+        # The default "step" of wandb is not the actual time step (gloabl_step) of the MDP
+        wandb.define_metric("*", step_metric="global_step")
+
+    def close_wandb(self):
+        import wandb
+        self.writer.close()
+        wandb.finish()
+
+    
+    def get_config(self) -> dict:
+        """Generates dictionary of the algorithm parameters configuration
+
+        Returns:
+            dict: Config
+        """
\ No newline at end of file
