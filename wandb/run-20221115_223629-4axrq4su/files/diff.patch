diff --git a/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc b/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc
index 904751d..6cb6a76 100644
Binary files a/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc and b/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc differ
diff --git a/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc b/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc
index 7521823..9514c0a 100644
Binary files a/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc and b/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc differ
diff --git a/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc b/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc
index cd5983c..833b0c9 100644
Binary files a/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc and b/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc differ
diff --git a/deepseatreasure-pareto/agent.py b/deepseatreasure-pareto/agent.py
index c689b43..0372c4d 100644
--- a/deepseatreasure-pareto/agent.py
+++ b/deepseatreasure-pareto/agent.py
@@ -13,6 +13,8 @@ from pygmo import hypervolume
 from metrics import metrics as met
 import deepst
 metrics = met([],[],[])
+import wandb
+
 
 class Pareto():
     def __init__(self, env,actionsMethods, choose_action, ref_point, nO=2,nS = 64, gamma=1.):
@@ -34,25 +36,34 @@ class Pareto():
         self.epsilonDecrease = 0.99
         self.stateList = []
 
-        self.polDict = {}
+        
         self.polIndex = 0
+        self.log = False
+        self.qtable = np.zeros((self.nS, self.nA, nO),dtype=object)
+
         
 
     def initializeState(self):
-        state = self.env.reset()
-        s = ''.join(str(state))
-        if s not in self.stateList:
-            self.stateList.append(s)
-        s = self.stateList.index(s)
-        
-        
-        
+        return self.flatten_observation(self.env.reset())
 
+    def flatten_observation(self, obs):
+        #print(obs[1])
         
-        return {'observation':s,'terminal':False}
+        if type(obs[1]) is dict:
+        	return int(np.ravel_multi_index((0,0), (11, 11)))
+        #print(type(obs[1]))
+           
+        else:
+            return int(np.ravel_multi_index(obs, (11, 11)))
+
 
+    def train(self,max_episodes,max_steps,log):
+        
+        self.polDict = np.zeros((max_episodes,self.nS, self.nA, 2))
+        self.log = log
+        if log:
+            metrics.setup_wandb("pql", "pql")
 
-    def train(self,max_episodes,max_steps):
         numberOfEpisodes = 0
         episodeSteps = 0
 
@@ -63,32 +74,149 @@ class Pareto():
 
             acumulatedRewards = [0,0]
             episodeSteps = 0
-
+            terminal = False
             #line 3 -> initialize state s
             s = self.initializeState()
             #print(s)
             
             
             #line 4 and 11 -> repeat until s is terminal:
-            while s['terminal'] is not True and episodeSteps < max_steps:
-                #env.render()
-                s = self.step(s)
+            while terminal is not True and episodeSteps < max_steps:
+                env.render()
+                s,terminal,reward = self.step(s)
                 #print(s, episodeSteps)
                 episodeSteps += 1
-                acumulatedRewards[0] += s['reward'][0]
-                acumulatedRewards[1] += s['reward'][1]
+                acumulatedRewards[0] += reward[0]
+                acumulatedRewards[1] += reward[1]
+
 
             metrics.rewards1.append(acumulatedRewards[0])
             metrics.rewards2.append(acumulatedRewards[1])
             metrics.episodes.append(numberOfEpisodes)
+            log_dict = {
+                    "reward0":acumulatedRewards[0],
+                    "reward1":acumulatedRewards[1],
+                    "episode":numberOfEpisodes
+
+
+
+                }
+            if log:
+                wandb.log(log_dict)
+            
+            
+            #print()
+            #self.qtable (estados,ações,objetivos)
+            
+            self.episodeqtable = copy.deepcopy(self.qtable)
+            self.polDict[numberOfEpisodes] = self.episodeqtable
+            self.polIndex +=1
+            
+            metrics.pdict = self.polDict
             numberOfEpisodes+=1
-            print(numberOfEpisodes)
+
+
+    ######################### PARETO TRAINING ################
+
+        print(self.polDict.shape)
+        
+        self.currentepisode = 0
+        q_set = self.polDict
+        numberOfEpisodes = 0
+        episodeSteps = 0
+
+        #line 1 -> initialize q_set
+        print("-> Pareto Training started <-")
+
+        #line 2 -> for each episode
+        
+        while self.currentepisode  < max_episodes:
+            print("Pareto ep:" + str(self.currentepisode))
+            ParetoacumulatedRewards = [0,0,0]
+            episodeSteps = 0
+            
+
+
+            #line 3 -> initialize state s
+            s = self.initializeState()
+            
+            terminalpareto = False
+            
+            #line 4 and 11 -> repeat until s is terminal:
+            while terminalpareto is not True and episodeSteps < max_steps:
+                s,terminalpareto,reward = self.stepPareto(s)
+                #print(s, episodeSteps)
+                episodeSteps += 1
+                ParetoacumulatedRewards[0] += reward[0]
+                ParetoacumulatedRewards[1] += reward[1]
+                
+            
+            log_dict2 = {
+                    "paretoreward0":ParetoacumulatedRewards[0],
+                    "paretoreward1":ParetoacumulatedRewards[1],
+                    "paretoepisode":self.currentepisode
+
+
+
+                }
+            if log:
+                wandb.log(log_dict2)
+
+            metrics.paretor0.append(ParetoacumulatedRewards[0])
+            metrics.paretor1.append(ParetoacumulatedRewards[1])
+
+
+
+            self.currentepisode+=1
+
+        if log:
+            data = [[x, y] for (x, y) in zip(metrics.paretor0, metrics.paretor1)]
+            table = wandb.Table(data=data, columns = ["x", "y"])
+            wandb.log({"my_custom_plot_id" : wandb.plot.scatter(table, "x", "y", title="Custom Y vs X Scatter Plot")})
+
+
+            metrics.close_wandb()
+
+
+    def stepPareto(self,s):
+        
+        self.actionsMethods.epsilon = 0 
+        
+        
+                
+            
+                
+        q_set = self.polDict[self.currentepisode][s]
         
-        metrics.pdict = self.polDict
+        q_set = q_set[:, np.newaxis,: ]
+                
+        action = self.choose_action(s, q_set)
+        next_state, reward, terminal, _ = self.env.step(action)
+        next_s = ''.join(str(next_state))
+        if next_s not in self.stateList:
+            self.stateList.append(next_s)
+        next_s = self.stateList.index(next_s)
+        nd = self.update_non_dominated(s, action, next_s)
+        self.n_visits[s, action] += 1
+        self.avg_r[s, action] += (reward - self.avg_r[s, action]) / self.n_visits[s, action] 
+        return next_s,terminal,reward
+        
+        
+
 
 
-    def step(self,state):
-        s = state['observation']
+
+
+
+
+
+
+
+        
+
+
+    def step(self,s):
+        
         
         
 
@@ -99,18 +227,16 @@ class Pareto():
         
         #metrics for pareto plot
         self.qcopy = copy.deepcopy(q_set)
-        self.polDict[self.polIndex] = self.qcopy
-        self.polIndex +=1
+
 
 
         #line 6 ->Take action a and observe state s0 ∈ S and reward vector r ∈ R
         next_state, reward, terminal, _ = self.env.step(action)
-        next_s = ''.join(str(next_state))
+        next_s = self.flatten_observation(next_state)
+        
+        
         
-        if next_s not in self.stateList:
-            self.stateList.append(next_s)
         
-        next_s = self.stateList.index(next_s)
         nd = self.update_non_dominated(s, action, next_s)
         
         
@@ -120,10 +246,9 @@ class Pareto():
         self.avg_r[s, action] += (reward - self.avg_r[s, action]) / self.n_visits[s, action]
 
         self.actionsMethods.epsilon *= self.actionsMethods.epsilonDecrease
+        
 
-        return {'observation': next_s,
-                'terminal': terminal,
-                'reward': reward}
+        return next_s,terminal,reward
 
     
     def compute_q_set(self, s):
@@ -131,12 +256,14 @@ class Pareto():
         for a in range(self.env.nA):
             nd_sa = self.non_dominated[s][a]
             rew = self.avg_r[s, a]
+            
+          
             q_set.append([rew + self.gamma*nd for nd in nd_sa])
         return np.array(q_set)
 
 
-    def update_non_dominated(self, s, a, s_n):
-        q_set_n = self.compute_q_set(s_n)
+    def update_non_dominated(self, s, a, next_state):
+        q_set_n = self.compute_q_set(next_state)
         # update for all actions, flatten
         solutions = np.concatenate(q_set_n, axis=0)
 
@@ -172,8 +299,10 @@ class actionMethods():
             q_values[i] = hv.compute(ref*-1)
         return q_values
 
+
     def get_non_dominated(self,solutions):
         is_efficient = np.ones(solutions.shape[0], dtype=bool)
+        
         for i, c in enumerate(solutions):
             if is_efficient[i]:
                 # Remove dominated points, will also remove itself
@@ -189,20 +318,20 @@ if __name__ == '__main__':
     from gym import wrappers
     from deepst import DeepSeaTreasure
     env = DeepSeaTreasure()
-    numberOfStates = 64
+    numberOfStates = 121
     numberOfObjectives = 2
     epsilon = 1
     epsilonDecrease = 0.999
     acMeth = actionMethods(epsilon,epsilonDecrease)
     ref_point = np.array([0, -25])
-    gamma = 0.9
+    gamma = 0.96
 
     #agent call
     agent = Pareto(env,acMeth, lambda s, q: acMeth.get_action(s, q, env), ref_point, nO=numberOfObjectives,nS = numberOfStates, gamma=gamma)
-    agent.train(2000,400)
+    agent.train(3500,1500,log = True)
 
     #metrics
-    metrics.plotGraph()
+    #metrics.plotGraph()
     metrics.plot_pareto_frontier()
 
     print("-> Done <-")
\ No newline at end of file
diff --git a/deepseatreasure-pareto/metrics.py b/deepseatreasure-pareto/metrics.py
index 8ca2346..27acd9e 100644
--- a/deepseatreasure-pareto/metrics.py
+++ b/deepseatreasure-pareto/metrics.py
@@ -4,12 +4,16 @@ from re import X
 import matplotlib.pyplot as plt
 import datetime
 import os
+import wandb
+from torch.utils.tensorboard import SummaryWriter
 
 class metrics():
     def __init__(self, episodes, rewards1, rewards2):
         self.episodes = episodes
         self.rewards1 = rewards1
         self.rewards2 = rewards2
+        self.paretor0 = []
+        self.paretor1 = []
         self.nonDominatedPoints = []
         self.ndPoints =[]
         self.pdict = {}
@@ -53,6 +57,10 @@ class metrics():
     def plot_p_front(self,Xs,Ys,actionIndex,maxY = True,maxX = True):
         
         
+        Xs = self.paretor0
+        
+
+        Ys = self.paretor1
         sorted_list = sorted([[Xs[i], Ys[i]] for i in range(len(Xs))], reverse=maxX)
         pareto_front = [sorted_list[0]]
         for pair in sorted_list[1:]:
@@ -90,20 +98,19 @@ class metrics():
             
         
 
-        print(best_y)
-        print(best_x)
+
         frontier = []
-        for p in best_x:
-            if p in best_y:
-                frontier.append(p)     
+                
+        for p in best_y:
+            if p in best_x:
+                frontier.append(p)      
             
         pf_X = [pair[0] for pair in frontier]
         pf_Y = [pair[1] for pair in frontier]    
         plt.scatter(Xs,Ys)
         plt.plot(pf_X, pf_Y)
-        plt.xlabel("Treasure Reward for Action " + str(actionIndex) )
-        plt.ylabel("Time Penalty for Action " + str(actionIndex))
-        plt.savefig(self.path + '//Pareto front - ' + "Treasure Reward" + ' x ' + " Time Penalty " + " for action " + str(actionIndex))    
+        plt.xlabel("Treasure Reward  " )
+        plt.ylabel("Time Penalty " )
 
         plt.show()
            
@@ -115,33 +122,138 @@ class metrics():
     def plot_pareto_frontier(self):
         '''Pareto frontier selection process'''
         
-        #print(self.pdict)
-        i = 0
+        #
+        self.plot_p_front(self.paretor0,self.paretor1,3)
+        
+        
+    def setup_wandb(self, project_name: str, experiment_name: str):
+        self.experiment_name = experiment_name
+        import wandb
+
+        wandb.init(
+            project=project_name,
+            sync_tensorboard=True,
+            config=self.get_config(),
+            name=self.experiment_name,
+            monitor_gym=True,
+            save_code=True,
+
+        )
+        self.writer = SummaryWriter(f"{self.experiment_name}")
+        # The default "step" of wandb is not the actual time step (gloabl_step) of the MDP
+        wandb.define_metric("*", step_metric="global_step")
+
+    def close_wandb(self):
+        import wandb
+        self.writer.close()
+        wandb.finish()
+
+    
+    def get_config(self) -> dict:
+        """Generates dictionary of the algorithm parameters configuration
+
+        Returns:
+            dict: Config
+        """
+
+    def plot_p_front2(self,Xs,Ys,actionIndex,maxY = True,maxX = True):
+        import numpy as np
+        """
+        sorted_list = sorted([[Xs[i], Ys[i],Zs[i]] for i in range(len(Xs))], reverse=maxX)
+        
+        pareto_front = [sorted_list[0]]
+        
+        
+        for pair in sorted_list[1:]:
+            #pareto_front.append(pair)
+            if maxY:
+               
+                if pair[1] >= pareto_front[-1][1]:
+                    
+                    pareto_front.append(pair)
+            else:
+                if pair[1] <= pareto_front[-1][1]:
+                    pareto_front.append(pair)
+     
+        
+        print(self.pdict)
+        """
+        frontier = []
+        
+        Xs = self.paretor0
+        Ys = self.paretor1
+        points = np.column_stack((Xs, Ys))
+        uniques = np.unique(points,axis=0)
+        
+        inputPoints = uniques.tolist()
+        paretoPoints, dominatedPoints = simple_cull(inputPoints, dominates)
+
+        print ("*"*8 + " non-dominated answers " + ("*"*8))
+        for p in paretoPoints:
+            frontier.append(p)
+            print (p)
+        print ("*"*8 + " dominated answers " + ("*"*8))
+        for p in dominatedPoints:
+            pass
+            #print (p)
+        #print(arr)
 
+        print(frontier)
         
-        #print(self.pdict[i])
-        c = self.pdict[i]
+  
+        pf_Xx = [pair[0] for pair in frontier]
+        pf_Yy = [pair[1] for pair in frontier]
+          
 
         
+      
         
-        for v in self.pdict.values():
-            self.xA0.append(v[0][0][0])
-            self.yA0.append(v[0][0][1])
-        for v in self.pdict.values():
-            self.xA1.append(v[1][0][0])
-            self.yA1.append(v[1][0][1])
-        for v in self.pdict.values():
-            self.xA2.append(v[2][0][0])
-            self.yA2.append(v[2][0][1])
-        for v in self.pdict.values():
-            self.xA3.append(v[3][0][0])
-            self.yA3.append(v[3][0][1])
         
-        #print(xA0)
-        self.plot_p_front(self.xA0,self.yA0,0)
-        self.plot_p_front(self.xA1,self.yA1,1)
-        self.plot_p_front(self.xA2,self.yA2,2)
-        self.plot_p_front(self.xA3,self.yA3,3)
+        Xs = points[:,0]
+        Ys = points[:,1]
+        
+
         
+        plt.plot(Xs,Ys)
+        plt.scatter(pf_Xx, pf_Yy)
+        plt.xlabel("Treasure Reward  " )
+        plt.ylabel("Time Penalty " )
         
-    
\ No newline at end of file
+
+        
+        
+        
+        
+               
+        plt.show()
+def simple_cull(inputPoints, dominates): 
+    paretoPoints = set()
+    candidateRowNr = 0
+    dominatedPoints = set()
+    while True:
+        candidateRow = inputPoints[candidateRowNr]
+        inputPoints.remove(candidateRow)
+        rowNr = 0
+        nonDominated = True
+        while len(inputPoints) != 0 and rowNr < len(inputPoints):
+            row = inputPoints[rowNr]
+            if dominates(candidateRow, row):
+                # If it is worse on all features remove the row from the array
+                inputPoints.remove(row)
+                dominatedPoints.add(tuple(row))
+            elif dominates(row, candidateRow):
+                nonDominated = False
+                dominatedPoints.add(tuple(candidateRow))
+                rowNr += 1
+            else:
+                rowNr += 1
+
+        if nonDominated:
+            # add the non-dominated point to the Pareto frontier
+            paretoPoints.add(tuple(candidateRow))
+
+        if len(inputPoints) == 0:
+            break
+    return paretoPoints, dominatedPoints
+def dominates(row, candidateRow):
+    return sum([row[x] >= candidateRow[x] for x in range(len(row))]) == len(row) 
\ No newline at end of file
