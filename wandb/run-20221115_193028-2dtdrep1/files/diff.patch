diff --git a/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc b/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc
index 904751d..3d5054a 100644
Binary files a/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc and b/deepseatreasure-pareto/__pycache__/agent.cpython-37.pyc differ
diff --git a/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc b/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc
index 7521823..9514c0a 100644
Binary files a/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc and b/deepseatreasure-pareto/__pycache__/deepst.cpython-37.pyc differ
diff --git a/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc b/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc
index cd5983c..80e70ff 100644
Binary files a/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc and b/deepseatreasure-pareto/__pycache__/metrics.cpython-37.pyc differ
diff --git a/deepseatreasure-pareto/agent.py b/deepseatreasure-pareto/agent.py
index c689b43..da53b32 100644
--- a/deepseatreasure-pareto/agent.py
+++ b/deepseatreasure-pareto/agent.py
@@ -13,6 +13,7 @@ from pygmo import hypervolume
 from metrics import metrics as met
 import deepst
 metrics = met([],[],[])
+import wandb
 
 class Pareto():
     def __init__(self, env,actionsMethods, choose_action, ref_point, nO=2,nS = 64, gamma=1.):
@@ -36,23 +37,29 @@ class Pareto():
 
         self.polDict = {}
         self.polIndex = 0
+        self.log = False
         
 
     def initializeState(self):
-        state = self.env.reset()
-        s = ''.join(str(state))
-        if s not in self.stateList:
-            self.stateList.append(s)
-        s = self.stateList.index(s)
-        
-        
-        
+        return self.flatten_observation(self.env.reset())
 
+    def flatten_observation(self, obs):
+        #print(obs[1])
         
-        return {'observation':s,'terminal':False}
+        if type(obs[1]) is dict:
+        	return int(np.ravel_multi_index((0,0), (11, 11)))
+        #print(type(obs[1]))
+           
+        else:
+            return int(np.ravel_multi_index(obs, (11, 11)))
+
 
+    def train(self,max_episodes,max_steps,log):
+        
+        self.log = log
+        if log:
+            metrics.setup_wandb("pql", "pql")
 
-    def train(self,max_episodes,max_steps):
         numberOfEpisodes = 0
         episodeSteps = 0
 
@@ -63,32 +70,133 @@ class Pareto():
 
             acumulatedRewards = [0,0]
             episodeSteps = 0
-
+            terminal = False
             #line 3 -> initialize state s
             s = self.initializeState()
             #print(s)
             
             
             #line 4 and 11 -> repeat until s is terminal:
-            while s['terminal'] is not True and episodeSteps < max_steps:
+            while terminal is not True and episodeSteps < max_steps:
                 #env.render()
-                s = self.step(s)
+                s,terminal,reward = self.step(s)
                 #print(s, episodeSteps)
                 episodeSteps += 1
-                acumulatedRewards[0] += s['reward'][0]
-                acumulatedRewards[1] += s['reward'][1]
+                acumulatedRewards[0] += reward[0]
+                acumulatedRewards[1] += reward[1]
+
 
             metrics.rewards1.append(acumulatedRewards[0])
             metrics.rewards2.append(acumulatedRewards[1])
             metrics.episodes.append(numberOfEpisodes)
+            log_dict = {
+                    "reward0":acumulatedRewards[0],
+                    "reward1":acumulatedRewards[1],
+                    "episode":numberOfEpisodes
+
+
+
+                }
+            if log:
+                wandb.log(log_dict)
             numberOfEpisodes+=1
             print(numberOfEpisodes)
+
+
+    ######################### PARETO TRAINING ################
+
+            self.episodeqtable = copy.deepcopy(self.qtable)
+            self.polDict[self.polIndex] = self.episodeqtable
+            self.polIndex +=1
+            metrics.pdict = self.polDict
+        self.currentepisode = 0
+        q_set = self.polDict
+        numberOfEpisodes = 0
+        episodeSteps = 0
+
+        #line 1 -> initialize q_set
+        print("-> Pareto Training started <-")
+
+        #line 2 -> for each episode
+        print(self.currentepisode)
         
-        metrics.pdict = self.polDict
+        while self.currentepisode  < max_episodes:
+            print(self.currentepisode)
+            ParetoacumulatedRewards = [0,0,0]
+            episodeSteps = 0
+            
+
+
+            #line 3 -> initialize state s
+            s = self.initializeState()
+            
+            
+            
+            #line 4 and 11 -> repeat until s is terminal:
+            while s['terminal'] is not True and episodeSteps < max_steps:
+                s = self.stepPareto(s)
+                #print(s, episodeSteps)
+                episodeSteps += 1
+                ParetoacumulatedRewards[0] += s['reward'][0]
+                ParetoacumulatedRewards[1] += s['reward'][1]
+                
+            
+            log_dict = {
+                    "reward0":ParetoacumulatedRewards[0],
+                    "reward1":ParetoacumulatedRewards[1],
+                    "episode":self.currentepisode
 
 
-    def step(self,state):
+
+                }
+
+
+            self.currentepisode+=1
+            if log:
+                metrics.close_wandb()
+
+    def stepPareto(self,state):
+        
+        self.actionsMethods.epsilon = 0 
+        
         s = state['observation']
+                
+            
+                
+        q_set = self.polDict[self.currentepisode][s]
+        
+        q_set = q_set[:, np.newaxis,: ]
+        print(q_set.shape)
+                
+        action = self.choose_action(s, q_set)
+        next_state, reward, terminal,inf,_= self.env.step(action)
+        next_s = ''.join(str(next_state))
+        if next_s not in self.stateList:
+            self.stateList.append(next_s)
+        next_s = self.stateList.index(next_s)
+        nd = self.update_non_dominated(s, action, next_s)
+        self.n_visits[s, action] += 1
+        self.avg_r[s, action] += (reward - self.avg_r[s, action]) / self.n_visits[s, action] 
+        return {'observation': next_s,
+                'terminal': terminal,
+                'reward': reward} 
+        
+        
+
+
+
+
+
+
+
+
+
+
+        
+
+
+    def step(self,s):
+        
         
         
 
@@ -105,12 +213,11 @@ class Pareto():
 
         #line 6 ->Take action a and observe state s0 ∈ S and reward vector r ∈ R
         next_state, reward, terminal, _ = self.env.step(action)
-        next_s = ''.join(str(next_state))
+        next_s = self.flatten_observation(next_state)
+        
+        
         
-        if next_s not in self.stateList:
-            self.stateList.append(next_s)
         
-        next_s = self.stateList.index(next_s)
         nd = self.update_non_dominated(s, action, next_s)
         
         
@@ -120,10 +227,9 @@ class Pareto():
         self.avg_r[s, action] += (reward - self.avg_r[s, action]) / self.n_visits[s, action]
 
         self.actionsMethods.epsilon *= self.actionsMethods.epsilonDecrease
+        
 
-        return {'observation': next_s,
-                'terminal': terminal,
-                'reward': reward}
+        return next_s,terminal,reward
 
     
     def compute_q_set(self, s):
@@ -131,12 +237,14 @@ class Pareto():
         for a in range(self.env.nA):
             nd_sa = self.non_dominated[s][a]
             rew = self.avg_r[s, a]
+            
+          
             q_set.append([rew + self.gamma*nd for nd in nd_sa])
         return np.array(q_set)
 
 
-    def update_non_dominated(self, s, a, s_n):
-        q_set_n = self.compute_q_set(s_n)
+    def update_non_dominated(self, s, a, next_state):
+        q_set_n = self.compute_q_set(next_state)
         # update for all actions, flatten
         solutions = np.concatenate(q_set_n, axis=0)
 
@@ -172,8 +280,10 @@ class actionMethods():
             q_values[i] = hv.compute(ref*-1)
         return q_values
 
+
     def get_non_dominated(self,solutions):
         is_efficient = np.ones(solutions.shape[0], dtype=bool)
+        
         for i, c in enumerate(solutions):
             if is_efficient[i]:
                 # Remove dominated points, will also remove itself
@@ -189,7 +299,7 @@ if __name__ == '__main__':
     from gym import wrappers
     from deepst import DeepSeaTreasure
     env = DeepSeaTreasure()
-    numberOfStates = 64
+    numberOfStates = 121
     numberOfObjectives = 2
     epsilon = 1
     epsilonDecrease = 0.999
@@ -199,10 +309,10 @@ if __name__ == '__main__':
 
     #agent call
     agent = Pareto(env,acMeth, lambda s, q: acMeth.get_action(s, q, env), ref_point, nO=numberOfObjectives,nS = numberOfStates, gamma=gamma)
-    agent.train(2000,400)
+    agent.train(2000,400,True)
 
     #metrics
     metrics.plotGraph()
-    metrics.plot_pareto_frontier()
+    #metrics.plot_pareto_frontier()
 
     print("-> Done <-")
\ No newline at end of file
diff --git a/deepseatreasure-pareto/metrics.py b/deepseatreasure-pareto/metrics.py
index 8ca2346..86fc63e 100644
--- a/deepseatreasure-pareto/metrics.py
+++ b/deepseatreasure-pareto/metrics.py
@@ -4,6 +4,8 @@ from re import X
 import matplotlib.pyplot as plt
 import datetime
 import os
+import wandb
+from torch.utils.tensorboard import SummaryWriter
 
 class metrics():
     def __init__(self, episodes, rewards1, rewards2):
@@ -144,4 +146,32 @@ class metrics():
         self.plot_p_front(self.xA3,self.yA3,3)
         
         
-    
\ No newline at end of file
+    def setup_wandb(self, project_name: str, experiment_name: str):
+        self.experiment_name = experiment_name
+        import wandb
+
+        wandb.init(
+            project=project_name,
+            sync_tensorboard=True,
+            config=self.get_config(),
+            name=self.experiment_name,
+            monitor_gym=True,
+            save_code=True,
+
+        )
+        self.writer = SummaryWriter(f"{self.experiment_name}")
+        # The default "step" of wandb is not the actual time step (gloabl_step) of the MDP
+        wandb.define_metric("*", step_metric="global_step")
+
+    def close_wandb(self):
+        import wandb
+        self.writer.close()
+        wandb.finish()
+
+    
+    def get_config(self) -> dict:
+        """Generates dictionary of the algorithm parameters configuration
+
+        Returns:
+            dict: Config
+        """
\ No newline at end of file
